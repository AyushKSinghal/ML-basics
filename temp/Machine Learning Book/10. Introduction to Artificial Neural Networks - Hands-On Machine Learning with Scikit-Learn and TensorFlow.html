<!--[if IE]><![endif]-->
<!DOCTYPE html>
<!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage"" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/hands-on-machine-learning/9781491962282/ch03.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="2309833"
  data-user-uuid="2d2acfb7-1cff-4dc7-9037-8ffbac19b02e"
  data-username="ayushksinghal"
  data-account-type="Trial"
  
  data-activated-trial-date="12/02/2017"


  data-archive="9781491962282"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch03.html"
  data-epub-title="Hands-On Machine Learning with Scikit-Learn and TensorFlow" data-debug=0 data-testing=0><![endif]-->
<!--[if gt IE 8]><!-->
<html class="js flexbox flexboxlegacy no-touch no-websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg no-zoom gr__safaribooksonline_com" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" "="" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781491962282/ch03.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="2309833" data-user-uuid="2d2acfb7-1cff-4dc7-9037-8ffbac19b02e" data-username="ayushksinghal" data-account-type="Trial" data-activated-trial-date="12/02/2017" data-archive="9781491962282" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch03.html" data-epub-title="Hands-On Machine Learning with Scikit-Learn and TensorFlow" data-debug="0" data-testing="0" style="" data-ember-extension="1" lang="en"><!--<![endif]--><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491962282"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/510f1a6865.js"></script><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/nr-spa-1044.js"></script><script type="text/javascript" async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/linkid.js"></script><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/1732687426968531.js" async=""></script><script async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/fbevents.js"></script><script type="text/javascript" async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/bat.js"></script><script type="text/javascript" async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/conversion_async.js"></script><script type="text/javascript" async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/insight.js"></script><script type="text/javascript" async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/conversion_async.js"></script><script async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/gtm.js"></script><script async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/analytics.js"></script><script type="text/javascript">(window.NREUM||(NREUM={})).loader_config={xpid:"VQQDUVVVGwACU1RUAQA="};window.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var o=e[n]={exports:{}};t[n][0].call(o.exports,function(e){var o=t[n][1][e];return r(o||e)},o,o.exports)}return e[n].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<n.length;o++)r(n[o]);return r}({1:[function(t,e,n){function r(t){try{c.console&&console.log(t)}catch(e){}}var o,i=t("ee"),a=t(19),c={};try{o=localStorage.getItem("__nr_flags").split(","),console&&"function"==typeof console.log&&(c.console=!0,o.indexOf("dev")!==-1&&(c.dev=!0),o.indexOf("nr_dev")!==-1&&(c.nrDev=!0))}catch(s){}c.nrDev&&i.on("internal-error",function(t){r(t.stack)}),c.dev&&i.on("fn-err",function(t,e,n){r(n.stack)}),c.dev&&(r("NR AGENT IN DEVELOPMENT MODE"),r("flags: "+a(c,function(t,e){return t}).join(", ")))},{}],2:[function(t,e,n){function r(t,e,n,r,o){try{d?d-=1:i("err",[o||new UncaughtException(t,e,n)])}catch(c){try{i("ierr",[c,s.now(),!0])}catch(u){}}return"function"==typeof f&&f.apply(this,a(arguments))}function UncaughtException(t,e,n){this.message=t||"Uncaught error with no additional information",this.sourceURL=e,this.line=n}function o(t){i("err",[t,s.now()])}var i=t("handle"),a=t(20),c=t("ee"),s=t("loader"),f=window.onerror,u=!1,d=0;s.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(p){"stack"in p&&(t(12),t(11),"addEventListener"in window&&t(6),s.xhrWrappable&&t(13),u=!0)}c.on("fn-start",function(t,e,n){u&&(d+=1)}),c.on("fn-err",function(t,e,n){u&&(this.thrown=!0,o(n))}),c.on("fn-end",function(){u&&!this.thrown&&d>0&&(d-=1)}),c.on("internal-error",function(t){i("ierr",[t,s.now(),!0])})},{}],3:[function(t,e,n){t("loader").features.ins=!0},{}],4:[function(t,e,n){function r(){C++,M=y.hash,this[u]=b.now()}function o(){C--,y.hash!==M&&i(0,!0);var t=b.now();this[l]=~~this[l]+t-this[u],this[d]=t}function i(t,e){E.emit("newURL",[""+y,e])}function a(t,e){t.on(e,function(){this[e]=b.now()})}var c="-start",s="-end",f="-body",u="fn"+c,d="fn"+s,p="cb"+c,h="cb"+s,l="jsTime",m="fetch",v="addEventListener",w=window,y=w.location,b=t("loader");if(w[v]&&b.xhrWrappable){var g=t(9),x=t(10),E=t(8),O=t(6),R=t(12),P=t(7),T=t(13),S=t("ee"),N=S.get("tracer");t(14),b.features.spa=!0;var M,j=w[v],C=0;S.on(u,r),S.on(p,r),S.on(d,o),S.on(h,o),S.buffer([u,d,"xhr-done","xhr-resolved"]),O.buffer([u]),R.buffer(["setTimeout"+s,"clearTimeout"+c,u]),T.buffer([u,"new-xhr","send-xhr"+c]),P.buffer([m+c,m+"-done",m+f+c,m+f+s]),E.buffer(["newURL"]),g.buffer([u]),x.buffer(["propagate",p,h,"executor-err","resolve"+c]),N.buffer([u,"no-"+u]),a(T,"send-xhr"+c),a(S,"xhr-resolved"),a(S,"xhr-done"),a(P,m+c),a(P,m+"-done"),E.on("pushState-end",i),E.on("replaceState-end",i),j("hashchange",i,!0),j("load",i,!0),j("popstate",function(){i(0,C>1)},!0)}},{}],5:[function(t,e,n){function r(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var o=t("ee"),i=t("handle"),a=t(12),c=t(11),s="learResourceTimings",f="addEventListener",u="resourcetimingbufferfull",d="bstResource",p="resource",h="-start",l="-end",m="fn"+h,v="fn"+l,w="bstTimer",y="pushState",b=t("loader");b.features.stn=!0,t(8);var g=NREUM.o.EV;o.on(m,function(t,e){var n=t[0];n instanceof g&&(this.bstStart=b.now())}),o.on(v,function(t,e){var n=t[0];n instanceof g&&i("bst",[n,e,this.bstStart,b.now()])}),a.on(m,function(t,e,n){this.bstStart=b.now(),this.bstType=n}),a.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),this.bstType])}),c.on(m,function(){this.bstStart=b.now()}),c.on(v,function(t,e){i(w,[e,this.bstStart,b.now(),"requestAnimationFrame"])}),o.on(y+h,function(t){this.time=b.now(),this.startPath=location.pathname+location.hash}),o.on(y+l,function(t){i("bstHist",[location.pathname+location.hash,this.startPath,this.time])}),f in window.performance&&(window.performance["c"+s]?window.performance[f](u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance["c"+s]()},!1):window.performance[f]("webkit"+u,function(t){i(d,[window.performance.getEntriesByType(p)]),window.performance["webkitC"+s]()},!1)),document[f]("scroll",r,{passive:!0}),document[f]("keypress",r,!1),document[f]("click",r,!1)}},{}],6:[function(t,e,n){function r(t){for(var e=t;e&&!e.hasOwnProperty(u);)e=Object.getPrototypeOf(e);e&&o(e)}function o(t){c.inPlace(t,[u,d],"-",i)}function i(t,e){return t[1]}var a=t("ee").get("events"),c=t(22)(a,!0),s=t("gos"),f=XMLHttpRequest,u="addEventListener",d="removeEventListener";e.exports=a,"getPrototypeOf"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+"-start",function(t,e){var n=t[1],r=s(n,"nr@wrapped",function(){function t(){if("function"==typeof n.handleEvent)return n.handleEvent.apply(n,arguments)}var e={object:t,"function":n}[typeof n];return e?c(e,"fn-",null,e.name||"anonymous"):n});this.wrapped=t[1]=r}),a.on(d+"-start",function(t){t[1]=this.wrapped||t[1]})},{}],7:[function(t,e,n){function r(t,e,n){var r=t[e];"function"==typeof r&&(t[e]=function(){var t=r.apply(this,arguments);return o.emit(n+"start",arguments,t),t.then(function(e){return o.emit(n+"end",[null,e],t),e},function(e){throw o.emit(n+"end",[e],t),e})})}var o=t("ee").get("fetch"),i=t(19);e.exports=o;var a=window,c="fetch-",s=c+"body-",f=["arrayBuffer","blob","json","text","formData"],u=a.Request,d=a.Response,p=a.fetch,h="prototype";u&&d&&p&&(i(f,function(t,e){r(u[h],e,s),r(d[h],e,s)}),r(a,"fetch",c),o.on(c+"end",function(t,e){var n=this;e?e.clone().arrayBuffer().then(function(t){n.rxSize=t.byteLength,o.emit(c+"done",[null,e],n)}):o.emit(c+"done",[t],n)}))},{}],8:[function(t,e,n){var r=t("ee").get("history"),o=t(22)(r);e.exports=r,o.inPlace(window.history,["pushState","replaceState"],"-")},{}],9:[function(t,e,n){var r=t("ee").get("mutation"),o=t(22)(r),i=NREUM.o.MO;e.exports=r,i&&(window.MutationObserver=function(t){return this instanceof i?new i(o(t,"fn-")):i.apply(this,arguments)},MutationObserver.prototype=i.prototype)},{}],10:[function(t,e,n){function r(t){var e=a.context(),n=c(t,"executor-",e),r=new f(n);return a.context(r).getCtx=function(){return e},a.emit("new-promise",[r,e],e),r}function o(t,e){return e}var i=t(22),a=t("ee").get("promise"),c=i(a),s=t(19),f=NREUM.o.PR;e.exports=a,f&&(window.Promise=r,["all","race"].forEach(function(t){var e=f[t];f[t]=function(n){function r(t){return function(){a.emit("propagate",[null,!o],i),o=o||!t}}var o=!1;s(n,function(e,n){Promise.resolve(n).then(r("all"===t),r(!1))});var i=e.apply(f,arguments),c=f.resolve(i);return c}}),["resolve","reject"].forEach(function(t){var e=f[t];f[t]=function(t){var n=e.apply(f,arguments);return t!==n&&a.emit("propagate",[t,!0],n),n}}),f.prototype["catch"]=function(t){return this.then(null,t)},f.prototype=Object.create(f.prototype,{constructor:{value:r}}),s(Object.getOwnPropertyNames(f),function(t,e){try{r[e]=f[e]}catch(n){}}),a.on("executor-start",function(t){t[0]=c(t[0],"resolve-",this),t[1]=c(t[1],"resolve-",this)}),a.on("executor-err",function(t,e,n){t[1](n)}),c.inPlace(f.prototype,["then"],"then-",o),a.on("then-start",function(t,e){this.promise=e,t[0]=c(t[0],"cb-",this),t[1]=c(t[1],"cb-",this)}),a.on("then-end",function(t,e,n){this.nextPromise=n;var r=this.promise;a.emit("propagate",[r,!0],n)}),a.on("cb-end",function(t,e,n){a.emit("propagate",[n,!0],this.nextPromise)}),a.on("propagate",function(t,e,n){this.getCtx&&!e||(this.getCtx=function(){if(t instanceof Promise)var e=a.context(t);return e&&e.getCtx?e.getCtx():this})}),r.toString=function(){return""+f})},{}],11:[function(t,e,n){var r=t("ee").get("raf"),o=t(22)(r),i="equestAnimationFrame";e.exports=r,o.inPlace(window,["r"+i,"mozR"+i,"webkitR"+i,"msR"+i],"raf-"),r.on("raf-start",function(t){t[0]=o(t[0],"fn-")})},{}],12:[function(t,e,n){function r(t,e,n){t[0]=a(t[0],"fn-",null,n)}function o(t,e,n){this.method=n,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],"fn-",this,n)}var i=t("ee").get("timer"),a=t(22)(i),c="setTimeout",s="setInterval",f="clearTimeout",u="-start",d="-";e.exports=i,a.inPlace(window,[c,"setImmediate"],c+d),a.inPlace(window,[s],s+d),a.inPlace(window,[f,"clearImmediate"],f+d),i.on(s+u,r),i.on(c+u,o)},{}],13:[function(t,e,n){function r(t,e){d.inPlace(e,["onreadystatechange"],"fn-",c)}function o(){var t=this,e=u.context(t);t.readyState>3&&!e.resolved&&(e.resolved=!0,u.emit("xhr-resolved",[],t)),d.inPlace(t,y,"fn-",c)}function i(t){b.push(t),l&&(x?x.then(a):v?v(a):(E=-E,O.data=E))}function a(){for(var t=0;t<b.length;t++)r([],b[t]);b.length&&(b=[])}function c(t,e){return e}function s(t,e){for(var n in t)e[n]=t[n];return e}t(6);var f=t("ee"),u=f.get("xhr"),d=t(22)(u),p=NREUM.o,h=p.XHR,l=p.MO,m=p.PR,v=p.SI,w="readystatechange",y=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],b=[];e.exports=u;var g=window.XMLHttpRequest=function(t){var e=new h(t);try{u.emit("new-xhr",[e],e),e.addEventListener(w,o,!1)}catch(n){try{u.emit("internal-error",[n])}catch(r){}}return e};if(s(h,g),g.prototype=h.prototype,d.inPlace(g.prototype,["open","send"],"-xhr-",c),u.on("send-xhr-start",function(t,e){r(t,e),i(e)}),u.on("open-xhr-start",r),l){var x=m&&m.resolve();if(!v&&!m){var E=1,O=document.createTextNode(E);new l(a).observe(O,{characterData:!0})}}else f.on("fn-end",function(t){t[0]&&t[0].type===w||a()})},{}],14:[function(t,e,n){function r(t){var e=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<d;r++)t.removeEventListener(u[r],this.listener,!1);if(!e.aborted){if(n.duration=a.now()-this.startTime,4===t.readyState){e.status=t.status;var i=o(t,this.lastSize);if(i&&(n.rxSize=i),this.sameOrigin){var s=t.getResponseHeader("X-NewRelic-App-Data");s&&(e.cat=s.split(", ").pop())}}else e.status=0;n.cbTime=this.cbTime,f.emit("xhr-done",[t],t),c("xhr",[e,n,this.startTime])}}}function o(t,e){var n=t.responseType;if("json"===n&&null!==e)return e;var r="arraybuffer"===n||"blob"===n||"json"===n?t.response:t.responseText;return l(r)}function i(t,e){var n=s(e),r=t.params;r.host=n.hostname+":"+n.port,r.pathname=n.pathname,t.sameOrigin=n.sameOrigin}var a=t("loader");if(a.xhrWrappable){var c=t("handle"),s=t(15),f=t("ee"),u=["load","error","abort","timeout"],d=u.length,p=t("id"),h=t(18),l=t(17),m=window.XMLHttpRequest;a.features.xhr=!0,t(13),f.on("new-xhr",function(t){var e=this;e.totalCbs=0,e.called=0,e.cbTime=0,e.end=r,e.ended=!1,e.xhrGuids={},e.lastSize=null,h&&(h>34||h<10)||window.opera||t.addEventListener("progress",function(t){e.lastSize=t.loaded},!1)}),f.on("open-xhr-start",function(t){this.params={method:t[0]},i(this,t[1]),this.metrics={}}),f.on("open-xhr-end",function(t,e){"loader_config"in NREUM&&"xpid"in NREUM.loader_config&&this.sameOrigin&&e.setRequestHeader("X-NewRelic-ID",NREUM.loader_config.xpid)}),f.on("send-xhr-start",function(t,e){var n=this.metrics,r=t[0],o=this;if(n&&r){var i=l(r);i&&(n.txSize=i)}this.startTime=a.now(),this.listener=function(t){try{"abort"===t.type&&(o.params.aborted=!0),("load"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof e.onload))&&o.end(e)}catch(n){try{f.emit("internal-error",[n])}catch(r){}}};for(var c=0;c<d;c++)e.addEventListener(u[c],this.listener,!1)}),f.on("xhr-cb-time",function(t,e,n){this.cbTime+=t,e?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof n.onload||this.end(n)}),f.on("xhr-load-added",function(t,e){var n=""+p(t)+!!e;this.xhrGuids&&!this.xhrGuids[n]&&(this.xhrGuids[n]=!0,this.totalCbs+=1)}),f.on("xhr-load-removed",function(t,e){var n=""+p(t)+!!e;this.xhrGuids&&this.xhrGuids[n]&&(delete this.xhrGuids[n],this.totalCbs-=1)}),f.on("addEventListener-end",function(t,e){e instanceof m&&"load"===t[0]&&f.emit("xhr-load-added",[t[1],t[2]],e)}),f.on("removeEventListener-end",function(t,e){e instanceof m&&"load"===t[0]&&f.emit("xhr-load-removed",[t[1],t[2]],e)}),f.on("fn-start",function(t,e,n){e instanceof m&&("onload"===n&&(this.onload=!0),("load"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=a.now()))}),f.on("fn-end",function(t,e){this.xhrCbStart&&f.emit("xhr-cb-time",[a.now()-this.xhrCbStart,this.onload,e],e)})}},{}],15:[function(t,e,n){e.exports=function(t){var e=document.createElement("a"),n=window.location,r={};e.href=t,r.port=e.port;var o=e.href.split("://");!r.port&&o[1]&&(r.port=o[1].split("/")[0].split("@").pop().split(":")[1]),r.port&&"0"!==r.port||(r.port="https"===o[0]?"443":"80"),r.hostname=e.hostname||n.hostname,r.pathname=e.pathname,r.protocol=o[0],"/"!==r.pathname.charAt(0)&&(r.pathname="/"+r.pathname);var i=!e.protocol||":"===e.protocol||e.protocol===n.protocol,a=e.hostname===document.domain&&e.port===n.port;return r.sameOrigin=i&&(!e.hostname||a),r}},{}],16:[function(t,e,n){function r(){}function o(t,e,n){return function(){return i(t,[f.now()].concat(c(arguments)),e?null:this,n),e?void 0:this}}var i=t("handle"),a=t(19),c=t(20),s=t("ee").get("tracer"),f=t("loader"),u=NREUM;"undefined"==typeof window.newrelic&&(newrelic=u);var d=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],p="api-",h=p+"ixn-";a(d,function(t,e){u[e]=o(p+e,!0,"api")}),u.addPageAction=o(p+"addPageAction",!0),u.setCurrentRouteName=o(p+"routeName",!0),e.exports=newrelic,u.interaction=function(){return(new r).get()};var l=r.prototype={createTracer:function(t,e){var n={},r=this,o="function"==typeof e;return i(h+"tracer",[f.now(),t,n],r),function(){if(s.emit((o?"":"no-")+"fn-start",[f.now(),r,o],n),o)try{return e.apply(this,arguments)}finally{s.emit("fn-end",[f.now()],n)}}}};a("setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(t,e){l[e]=o(h+e)}),newrelic.noticeError=function(t){"string"==typeof t&&(t=new Error(t)),i("err",[t,f.now()])}},{}],17:[function(t,e,n){e.exports=function(t){if("string"==typeof t&&t.length)return t.length;if("object"==typeof t){if("undefined"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if("undefined"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!("undefined"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(e){return}}}},{}],18:[function(t,e,n){var r=0,o=navigator.userAgent.match(/Firefox[\/\s](\d+\.\d+)/);o&&(r=+o[1]),e.exports=r},{}],19:[function(t,e,n){function r(t,e){var n=[],r="",i=0;for(r in t)o.call(t,r)&&(n[i]=e(r,t[r]),i+=1);return n}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],20:[function(t,e,n){function r(t,e,n){e||(e=0),"undefined"==typeof n&&(n=t?t.length:0);for(var r=-1,o=n-e||0,i=Array(o<0?0:o);++r<o;)i[r]=t[e+r];return i}e.exports=r},{}],21:[function(t,e,n){e.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],22:[function(t,e,n){function r(t){return!(t&&t instanceof Function&&t.apply&&!t[a])}var o=t("ee"),i=t(20),a="nr@original",c=Object.prototype.hasOwnProperty,s=!1;e.exports=function(t,e){function n(t,e,n,o){function nrWrapper(){var r,a,c,s;try{a=this,r=i(arguments),c="function"==typeof n?n(r,a):n||{}}catch(f){p([f,"",[r,a,o],c])}u(e+"start",[r,a,o],c);try{return s=t.apply(a,r)}catch(d){throw u(e+"err",[r,a,d],c),d}finally{u(e+"end",[r,a,s],c)}}return r(t)?t:(e||(e=""),nrWrapper[a]=t,d(t,nrWrapper),nrWrapper)}function f(t,e,o,i){o||(o="");var a,c,s,f="-"===o.charAt(0);for(s=0;s<e.length;s++)c=e[s],a=t[c],r(a)||(t[c]=n(a,f?c+o:o,i,c))}function u(n,r,o){if(!s||e){var i=s;s=!0;try{t.emit(n,r,o,e)}catch(a){p([a,n,r,o])}s=i}}function d(t,e){if(Object.defineProperty&&Object.keys)try{var n=Object.keys(t);return n.forEach(function(n){Object.defineProperty(e,n,{get:function(){return t[n]},set:function(e){return t[n]=e,e}})}),e}catch(r){p([r])}for(var o in t)c.call(t,o)&&(e[o]=t[o]);return e}function p(e){try{t.emit("internal-error",e)}catch(n){}}return t||(t=o),n.inPlace=f,n.flag=a,n}},{}],ee:[function(t,e,n){function r(){}function o(t){function e(t){return t&&t instanceof r?t:t?s(t,c,i):i()}function n(n,r,o,i){if(!p.aborted||i){t&&t(n,r,o);for(var a=e(o),c=l(n),s=c.length,f=0;f<s;f++)c[f].apply(a,r);var d=u[y[n]];return d&&d.push([b,n,r,a]),a}}function h(t,e){w[t]=l(t).concat(e)}function l(t){return w[t]||[]}function m(t){return d[t]=d[t]||o(n)}function v(t,e){f(t,function(t,n){e=e||"feature",y[n]=e,e in u||(u[e]=[])})}var w={},y={},b={on:h,emit:n,get:m,listeners:l,context:e,buffer:v,abort:a,aborted:!1};return b}function i(){return new r}function a(){(u.api||u.feature)&&(p.aborted=!0,u=p.backlog={})}var c="nr@context",s=t("gos"),f=t(19),u={},d={},p=e.exports=o();p.backlog=u},{}],gos:[function(t,e,n){function r(t,e,n){if(o.call(t,e))return t[e];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,e,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[e]=r,r}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(t,e,n){function r(t,e,n,r){o.buffer([t],r),o.emit(t,e,n)}var o=t("ee").get("handle");e.exports=r,r.ee=o},{}],id:[function(t,e,n){function r(t){var e=typeof t;return!t||"object"!==e&&"function"!==e?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i="nr@id",a=t("gos");e.exports=r},{}],loader:[function(t,e,n){function r(){if(!x++){var t=g.info=NREUM.info,e=p.getElementsByTagName("script")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&e))return u.abort();f(y,function(e,n){t[e]||(t[e]=n)}),s("mark",["onload",a()+g.offset],null,"api");var n=p.createElement("script");n.src="https://"+t.agent,e.parentNode.insertBefore(n,e)}}function o(){"complete"===p.readyState&&i()}function i(){s("mark",["domContent",a()+g.offset],null,"api")}function a(){return E.exists&&performance.now?Math.round(performance.now()):(c=Math.max((new Date).getTime(),c))-g.offset}var c=(new Date).getTime(),s=t("handle"),f=t(19),u=t("ee"),d=window,p=d.document,h="addEventListener",l="attachEvent",m=d.XMLHttpRequest,v=m&&m.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:m,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var w=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-spa-1044.min.js"},b=m&&v&&v[h]&&!/CriOS/.test(navigator.userAgent),g=e.exports={offset:c,now:a,origin:w,features:{},xhrWrappable:b};t(16),p[h]?(p[h]("DOMContentLoaded",i,!1),d[h]("load",r,!1)):(p[l]("onreadystatechange",o),d[l]("onload",r)),s("mark",["firstbyte",c],null,"api");var x=0,E=t(21)},{}]},{},["loader",2,14,5,3,4]);</script><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.8cc2fd27400e.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/css.css" rel="stylesheet" type="text/css"><title>10. Introduction to Artificial Neural Networks - Hands-On Machine Learning with Scikit-Learn and TensorFlow</title><link rel="stylesheet" href="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/d6ec1592ffb3.css" type="text/css"><link rel="stylesheet" type="text/css" href="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/annotator.css"><link rel="stylesheet" href="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/font-awesome.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0}#sbo-rt-content div.preface p.byline+p.byline{margin:0}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding-top:10px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:1.5em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index li{line-height:140%}#sbo-rt-content div.index a.indexterm{color:#8e0012}#sbo-rt-content div.index ul,#sbo-rt-content div[data-type="index"] ul{list-style-type:none;padding-left:0;margin-left:0}#sbo-rt-content div.index ul li{padding-left:0;margin-left:0}#sbo-rt-content div.index ul li ul li{margin-left:1em}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781491962282/chapter/ch03.html",
          "book_id": "9781491962282",
          "chapter_uri": "ch03.html",
          "position": 1.94296272015,
          "user_uuid": "2d2acfb7-1cff-4dc7-9037-8ffbac19b02e",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781491962282/ch04.html"
        
      },
      title: "Hands\u002DOn Machine Learning with Scikit\u002DLearn and TensorFlow",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: false,
      show_ios_app_teaser: false
    };
    // ]]></script><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/modernizr.js"></script><script>
    
      
        

        

        
          
            window.PUBLIC_ANNOTATIONS = true;
          
        

      

      
        window.MOBILE_PUBLIC_ANNOTATIONS = false;
      

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

    
      window.PRIVACY_CONTROL_SWITCH = true;
    

    
      window.PUBLISHER_PAGES = true;
    

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "https://www.safaribooksonline.com/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://www.safaribooksonline.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": true
        }
      };
  </script><link rel="canonical" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html"><meta name="description" content=" Chapter 3. Classification In Chapter&nbsp;1 we mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes). In Chapter&nbsp;2 we explored a regression ... "><meta property="og:title" content="3. Classification"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781491962282/"><meta itemprop="name" content="3. Classification"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491962282/"><meta property="og:description" itemprop="description" content=" Chapter 3. Classification In Chapter&nbsp;1 we mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes). In Chapter&nbsp;2 we explored a regression ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781491962299"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta property="og:book:tag" itemprop="about" content="Core Programming"><meta property="og:book:tag" itemprop="about" content="Engineering"><meta property="og:book:tag" itemprop="about" content="Python"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
  (function(i,s,o,g,r,a,m) {
    i['GoogleAnalyticsObject']=r;
    i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
    a=s.createElement(o),m=s.getElementsByTagName(o)[0];
    a.async=1;
    a.src=g;
    m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

  if (matches && matches.length === 2) {
    user_uuid = matches[1];
  }


  ga('create', 'UA-39299553-7', {'userId': '2d2acfb7-1cff-4dc7-9037-8ffbac19b02e' });



  
    ga('set', 'dimension1', 'Trial');
  


ga('set', 'dimension6', user_uuid);


  ga('set', 'dimension2', '2d2acfb7-1cff-4dc7-9037-8ffbac19b02e');
  






//enable enhanced link tracking
ga('require', 'linkid', 'linkid.js');

// reading interface will track pageviews itself
if (document.location.pathname.indexOf("/library/view") !== 0) {
  ga('send', 'pageview');
}
</script><script>
    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
  </script><script defer="defer" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/vendor.js"></script><script defer="defer" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/reader.js"></script><script async="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/MathJax.js"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a_003.js"></script><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a_002.js"></script></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>
  



    
      <div class="working hide" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/s/" class="t-queue-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>queue icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 29.2C25.4 29.2 25.8 29.1 26.1 28.9L48.7 16.8C49.5 16.4 50 15.5 50 14.6 50 13.7 49.5 12.8 48.7 12.4L26.1 0.3C25.4-0.1 24.6-0.1 23.9 0.3L1.3 12.4C0.5 12.8 0 13.7 0 14.6 0 15.5 0.5 16.4 1.3 16.8L23.9 28.9C24.2 29.1 24.6 29.2 25 29.2ZM7.3 14.6L25 5.2 42.7 14.6 25 24 7.3 14.6ZM48.7 22.4L47.7 21.9 25 34.2 2.3 21.9 1.3 22.4C0.5 22.9 0 23.7 0 24.7 0 25.6 0.5 26.5 1.3 26.9L23.9 39.3C24.2 39.5 24.6 39.6 25 39.6 25.4 39.6 25.8 39.5 26.1 39.3L48.7 26.9C49.5 26.5 50 25.6 50 24.7 50 23.7 49.5 22.9 48.7 22.4ZM48.7 32.8L47.7 32.3 25 44.6 2.3 32.3 1.3 32.8C0.5 33.3 0 34.1 0 35.1 0 36 0.5 36.9 1.3 37.3L23.9 49.7C24.2 49.9 24.6 50 25 50 25.4 50 25.8 49.9 26.1 49.7L48.7 37.3C49.5 36.9 50 36 50 35.1 50 34.1 49.5 33.3 48.7 32.8Z"></path></g></svg><span>
                  Queue
              </span></a></li><li class="search"><a href="#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://www.safaribooksonline.com/oreilly-newsletters/" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/0011N00001APXw3QAH/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-On Machine Learning with Scikit-Learn and TensorFlow
      
    </h1></span></a><div class="toc-contents" style="max-height: 0px;">
  <div class="sbo-toc ">
    <button type="button" class="sbo-toc-thumb close"><div class="visuallyhidden">Close</div></button>
      <section class="ios-app-teaser">
        <ul>
            <li><a class="js-toc-link toc-link" href="https://itunes.apple.com/gb/app/safari-queue-library-over/id881697395?mt=8" role="button">Install App</a></li>
            <li><a class="js-toc-link toc-link" href="safaridetail://9781491962282" role="button">Open in App</a></li>
        </ul>
      </section>
      <div class="sbo-book-meta">
        
        <span class="cover">
         <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/">
          <img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a.jpg" alt="Cover image for Hands-On Machine Learning with Scikit-Learn and TensorFlow" width="140" height="184">
        </a>
        </span>
        <span class="title">
          
            
                <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/publisher/oreilly-media-inc/">
                  <img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/ORM_logo_box_rgb.png" class="publisher-logo video" alt="publisher logo">
                </a>
            
          

          <a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>
        </span>
        
        <span class="authors">by Aurélien Géron</span>
        

        
        <span class="publishers t-publishers">Published by
          <!-- Show publisher page link if publisher pages switch is on -->
          
            <a class="t-publisher-link toc-link js-toc-link" href="https://www.safaribooksonline.com/library/publisher/oreilly-media-inc/">
              O'Reilly Media, Inc.</a>, 2017
          
        </span>
        

    

    </div>
  <ol class="tocList">
    
    
    
     

     <li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/preface01.html#idm140583011384384">
        Preface 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/part01.html#fundamentals_part">
        I. The Fundamentals of Machine Learning 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1 currently-reading">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch01.html#landscape_chapter">
        1. The Machine Learning Landscape 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch02.html#project_chapter">
        2. End-to-End Machine Learning Project 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html#classification_chapter">
        3. Classification 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html#linear_models_chapter">
        4. Training Models 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch05.html#svm_chapter">
        5. Support Vector Machines 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch06.html#trees_chapter">
        6. Decision Trees 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch07.html#ensembles_chapter">
        7. Ensemble Learning and Random Forests 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch08.html#dim_reduction_chapter">
        8. Dimensionality Reduction 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/part02.html#neural_nets_part">
        II. Neural Networks and Deep Learning 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">
        9. Up and Running with TensorFlow 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#ann_chapter">
        10. Introduction to Artificial Neural Networks 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">
        11. Training Deep Neural Nets 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch12.html#distributed_chapter">
        12. Distributing TensorFlow Across Devices and Servers 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch13.html#cnn_chapter">
        13. Convolutional Neural Networks 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch14.html#rnn_chapter">
        14. Recurrent Neural Networks 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch15.html#autoencoders_chapter">
        15. Autoencoders 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch16.html#rl_chapter">
        16. Reinforcement Learning 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app01.html#solutions_appendix">
        A. Exercise Solutions 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app02.html#project_checklist_appendix">
        B. Machine Learning Project Checklist 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app03.html#svm_dual_problem_appendix">
        C. SVM Dual Problem 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app04.html#autodiff_appendix">
        D. Autodiff 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app05.html#other_ann_appendix">
        E. Other Popular ANN Architectures 
       </a>
      
        
      
    
    
     

     </li><li class="toc-level1">
        
        <a class="js-toc-link toc-link" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ix01.html#idm140582977822192">
        Index 
       </a>
      
        
      
   </li></ol>
 </div>



</div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input name="query" placeholder="Search inside this book..." autocomplete="off" type="search"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781491962282/chapter/ch10.html" data-for-analytics="9781491962282:ch10.html" aria-label="Add to Queue"><span>Add to Queue</span></button></li><li class="js-font-control-panel font-control-activator"><a href="#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html&amp;text=Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%203.%20Classification&amp;body=https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html%0D%0Afrom%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">9. Up and Running with TensorFlow</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">11. Training Deep Neural Nets</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Introduction to Artificial Neural Networks"><div class="chapter" id="ann_chapter">
<h1><span class="label">Chapter 10. </span>Introduction to Artificial Neural Networks</h1>


<p>Birds <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" id="ann10"></a><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="overview" id="ann10o"></a>inspired
 us to fly, burdock plants inspired velcro, and nature has inspired many
 other inventions. It seems only logical, then, to look at the brain’s 
architecture for inspiration on how to build an intelligent machine. 
This is the key idea that inspired <em>artificial neural networks</em> 
(ANNs). However, although planes were inspired by birds, they don’t have
 to flap their wings. Similarly, ANNs have gradually become quite 
different from their biological cousins. Some researchers even argue 
that we should drop the biological analogy altogether (e.g., by saying 
“units” rather than “neurons”), lest we restrict our creativity to 
biologically plausible systems.<sup><a data-type="noteref" id="idm140583002295248-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002295248" class="totri-footnote">1</a></sup></p>

<p>ANNs are at the very core of Deep Learning. They are versatile, 
powerful, and scalable, making them ideal to tackle large and highly 
complex Machine Learning tasks, such as classifying billions of images 
(e.g., Google Images), <a data-type="indexterm" data-primary="Google Images" id="idm140583002293696"></a>powering speech recognition services (e.g., Apple’s Siri), <a data-type="indexterm" data-primary="Apple’s Siri" id="idm140583002292864"></a>recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube), <a data-type="indexterm" data-primary="YouTube" id="idm140583002291952"></a>or learning to beat the world champion at the game of <em>Go</em> by examining millions of past games and then playing against <a data-type="indexterm" data-primary="DeepMind" id="idm140583002290704"></a> <a data-type="indexterm" data-primary="AlphaGo" id="idm140583002289872"></a>itself (DeepMind’s AlphaGo).</p>

<p>In this chapter, we will introduce artificial neural networks, 
starting with a quick tour of the very first ANN architectures. Then we 
will present <em>Multi-Layer Perceptrons</em> (MLPs) <a data-type="indexterm" data-primary="Multi-Layer Perceptrons (MLP)" id="idm140583002288208"></a>and implement one using TensorFlow to tackle the MNIST digit classification problem (introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html#classification_chapter">Chapter&nbsp;3</a>).</p>






<section data-type="sect1" data-pdf-bookmark="From Biological to Artificial Neurons"><div class="sect1" id="idm140583002286240">
<h1>From Biological to Artificial Neurons</h1>

<p>Surprisingly, <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="evolution of" id="idm140583002284480"></a><a data-type="indexterm" data-primary="neurons" data-secondary="biological" id="n10b"></a><a data-type="indexterm" data-primary="biological neurons" id="bn10"></a>ANNs
 have been around for quite a while: they were first introduced back in 
1943 by the neurophysiologist Warren McCulloch and the mathematician 
Walter Pitts. In their <a href="https://goo.gl/Ul4mxW">landmark paper</a>,<sup><a data-type="noteref" id="idm140583002280320-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002280320" class="totri-footnote">2</a></sup>
 “A Logical Calculus of Ideas Immanent in Nervous Activity,” McCulloch 
and Pitts presented a simplified computational model of how biological 
neurons might work together in animal brains to perform complex 
computations using <em>propositional logic</em>. <a data-type="indexterm" data-primary="propositional logic" id="idm140583002278864"></a>This
 was the first artificial neural network architecture. Since then many 
other architectures have been invented, as we will see.</p>

<p>The early successes of ANNs until the 1960s led to the widespread 
belief that we would soon be conversing with truly intelligent machines.
 When it became clear that this promise would go unfulfilled (at least 
for quite a while), funding flew elsewhere and ANNs entered a long dark 
era. In the early 1980s there was a revival of interest in ANNs as new 
network architectures were invented and better training techniques were 
developed. But by the 1990s, powerful alternative Machine Learning 
techniques such as Support Vector Machines (see <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch05.html#svm_chapter">Chapter&nbsp;5</a>)
 were favored by most researchers, as they seemed to offer better 
results and stronger theoretical foundations. Finally, we are now 
witnessing yet another wave of interest in ANNs. Will this wave die out 
like the previous ones did? There are a few good reasons to believe that
 this one is different and will have a much more profound impact on our 
lives:</p>

<ul>
<li>
<p>There is now a huge quantity of data available to train neural 
networks, and ANNs frequently outperform other ML techniques on very 
large and complex problems.</p>
</li>
<li>
<p>The tremendous increase in computing power since the 1990s now makes 
it possible to train large neural networks in a reasonable amount of 
time. This is in part due to Moore’s Law, but also thanks to the gaming 
industry, which has produced powerful GPU cards by the millions.</p>
</li>
<li>
<p>The training algorithms have been improved. To be fair they are only 
slightly different from the ones used in the 1990s, but these relatively
 small tweaks have a huge positive impact.</p>
</li>
<li>
<p>Some theoretical limitations of ANNs have turned out to be benign in 
practice. For example, many people thought that ANN training algorithms 
were doomed because they were likely to get stuck in local optima, but 
it turns out that this is rather rare in practice (or when it is the 
case, they are usually fairly close to the global optimum).</p>
</li>
<li>
<p>ANNs seem to have entered a virtuous circle of funding and progress. 
Amazing products based on ANNs regularly make the headline news, which 
pulls more and more attention and funding toward them, resulting in more
 and more progress, and even more amazing <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="overview" data-startref="ann10o" id="idm140583002269856"></a>products.</p>
</li>
</ul>








<section data-type="sect2" data-pdf-bookmark="Biological Neurons"><div class="sect2" id="idm140583002268256">
<h2>Biological Neurons</h2>

<p>Before we discuss artificial neurons, let’s take a quick look at a biological neuron (represented in <a data-type="xref" href="#biological_neuron">Figure&nbsp;10-1</a>). It is an unusual-looking cell mostly found in animal cerebral cortexes (e.g., your brain), composed of a <em>cell body</em> containing the nucleus and most of the cell’s complex components, and many branching extensions called <em>dendrites</em>, plus one very long extension called the <em>axon</em>.
 The axon’s length may be just a few times longer than the cell body, or
 up to tens of thousands of times longer. Near its extremity the axon 
splits off into many branches called <em>telodendria</em>, and at the tip of these branches are minuscule structures called <em>synaptic terminals</em> (or simply <em>synapses</em>),
 which are connected to the dendrites (or directly to the cell body) of 
other neurons. Biological neurons receive short electrical impulses 
called <em>signals</em> from other neurons via these synapses. When a 
neuron receives a sufficient number of signals from other neurons within
 a few milliseconds, it fires its own signals.</p>

<figure class="smallerseventy"><div id="biological_neuron" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1001.png" alt="mlst 1001" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1001.png" width="1440" height="933">
<h6><span class="label">Figure 10-1. </span>Biological neuron<sup><a data-type="noteref" id="idm140583002259840-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002259840" class="totri-footnote">3</a></sup></h6>
</div></figure>

<p>Thus, individual biological neurons seem to behave in a rather simple
 way, but they are organized in a vast network of billions of neurons, 
each neuron typically connected to thousands of other neurons. Highly 
complex computations can be performed by a vast network of fairly simple
 neurons, much like a complex anthill can emerge from the combined 
efforts of simple ants. The architecture of biological neural networks 
(BNN)<sup><a data-type="noteref" id="idm140583002256368-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002256368" class="totri-footnote">4</a></sup>
 is still the subject of active research, but some parts of the brain 
have been mapped, and it seems that neurons are often organized in 
consecutive layers, as <a data-type="indexterm" data-primary="biological neurons" data-startref="bn10" id="idm140583002255408"></a><a data-type="indexterm" data-primary="neurons" data-secondary="biological" data-startref="n10b" id="idm140583002254464"></a>shown in <a data-type="xref" href="#biological_neural_network">Figure&nbsp;10-2</a>.</p>

<figure><div id="biological_neural_network" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1002.png" alt="mlst 1002" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1002.png" width="743" height="238">
<h6><span class="label">Figure 10-2. </span>Multiple layers in a biological neural network (human cortex)<sup><a data-type="noteref" id="idm140583002250368-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002250368" class="totri-footnote">5</a></sup></h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Logical Computations with Neurons"><div class="sect2" id="idm140583002248320">
<h2>Logical Computations with Neurons</h2>

<p>Warren McCulloch <a data-type="indexterm" data-primary="neurons" data-secondary="logical computations with" id="n10lcw"></a>and Walter Pitts proposed a very simple model of the biological neuron, which later became known as <a data-type="indexterm" data-primary="artificial neuron" data-seealso="artificial neural network (ANN)" id="idm140583002245312"></a>an <em>artificial neuron</em>:
 it has one or more binary (on/off) inputs and one binary output. The 
artificial neuron simply activates its output when more than a certain 
number of its inputs are active. McCulloch and Pitts showed that even 
with such a simplified model it is possible to build a network of 
artificial neurons that computes any logical proposition you want. For 
example, let’s build a few ANNs that perform various logical 
computations (see <a data-type="xref" href="#nn_propositional_logic_diagram">Figure&nbsp;10-3</a>), assuming that a neuron is activated when at least two of its inputs are active.</p>

<figure class="smallereighty"><div id="nn_propositional_logic_diagram" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1003.png" alt="mlst 1003" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1003.png" width="1440" height="486">
<h6><span class="label">Figure 10-3. </span>ANNs performing simple logical computations</h6>
</div></figure>

<ul>
<li>
<p>The first network on the left is simply the identity function: if 
neuron A is activated, then neuron C gets activated as well (since it 
receives two input signals from neuron A), but if neuron A is off, then 
neuron C is off as well.</p>
</li>
<li>
<p>The second network performs a logical AND: neuron C is activated only
 when both neurons A and B are activated (a single input signal is not 
enough to activate neuron C).</p>
</li>
<li>
<p>The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).</p>
</li>
<li>
<p>Finally, if we suppose that an input connection can inhibit the 
neuron’s activity (which is the case with biological neurons), then the 
fourth network computes a slightly more complex logical proposition: 
neuron C is activated only if neuron A is active and if neuron B is off.
 If neuron A is active all the time, then you get a logical NOT: neuron C
 is active when neuron B is off, and vice versa.</p>
</li>
</ul>

<p>You can easily imagine how these networks can be combined to compute 
complex logical expressions (see the exercises at the end of the 
chapter).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Perceptron"><div class="sect2" id="idm140583002234688">
<h2>The Perceptron</h2>

<p>The <em>Perceptron</em> <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="Perceptrons" data-secondary-sortas="Perceptron" id="ann10tp"></a><a data-type="indexterm" data-primary="Perceptrons" id="p10"></a>is
 one of the simplest ANN architectures, invented in 1957 by Frank 
Rosenblatt. It is based on a slightly different artificial neuron (see <a data-type="xref" href="#artificial_neuron_diagram">Figure&nbsp;10-4</a>) called <a data-type="indexterm" data-primary="linear threshold units (LTUs)" id="ltu10"></a>a <em>linear threshold unit</em>
 (LTU): the inputs and output are now numbers (instead of binary on/off 
values) and each input connection is associated with a weight. The LTU 
computes a weighted sum of its inputs (<em>z</em> = <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> + <em>w</em><sub>2</sub> <em>x</em><sub>2</sub> + ⋯ + <em>w</em><sub><em>n</em></sub> <em>x</em><sub><em>n</em></sub> = <strong>w</strong><sup><em>T</em></sup> · <strong>x</strong>), then applies a <em>step function</em> <a data-type="indexterm" data-primary="step functions" id="idm140583002219312"></a>to that sum and outputs the result: <em>h</em><sub><strong>w</strong></sub>(<strong>x</strong>) = step (<em>z</em>) = step (<strong>w</strong><sup><em>T</em></sup> · <strong>x</strong>).</p>

<figure class="smallerfiftyfive"><div id="artificial_neuron_diagram" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1004.png" alt="mlst 1004" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1004.png" width="1145" height="613">
<h6><span class="label">Figure 10-4. </span>Linear threshold unit</h6>
</div></figure>

<p>The most common step function used in Perceptrons is <a data-type="indexterm" data-primary="Heaviside step function" id="idm140583002213312"></a>the <em>Heaviside step function</em> (see <a data-type="xref" href="#step_functions_equation">Equation 10-1</a>). Sometimes the sign function is used instead.</p>
<div id="step_functions_equation" data-type="equation" class="pagebreak-before less_space">
<h5><span class="label">Equation 10-1. </span>Common step functions used in Perceptrons</h5>
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/eq_98.png" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/eq_98.png" width="1496" height="189">
</div>

<p>A single LTU can be used for simple linear binary classification. It 
computes a linear combination of the inputs and if the result exceeds a 
threshold, it outputs the positive class or else outputs the negative 
class (just like a Logistic Regression classifier or a linear SVM). For 
example, you could use a single LTU to classify iris flowers based on 
the petal length and width (also adding an extra bias feature <em>x</em><sub>0</sub> = 1, just like we did in previous chapters). Training an LTU means finding the right values for <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>2</sub> (the training algorithm is discussed shortly).</p>

<p>A Perceptron is simply composed of a single layer of LTUs,<sup><a data-type="noteref" id="idm140583002204192-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002204192" class="totri-footnote">6</a></sup> with each neuron connected to all the inputs. These connections are often represented using special passthrough neurons <a data-type="indexterm" data-primary="input neurons" id="idm140583002202912"></a>called <em>input neurons</em>: they just output whatever input they are fed. Moreover, an extra bias feature is generally added (<em>x</em><sub>0</sub> = 1). This bias feature is typically represented using a special type of neuron called a <em>bias neuron</em>, <a data-type="indexterm" data-primary="bias neurons" id="idm140583002200272"></a>which just outputs 1 all the time.</p>

<p>A Perceptron with two inputs and three outputs is represented in <a data-type="xref" href="#perceptron_diagram">Figure&nbsp;10-5</a>.
 This Perceptron can classify instances simultaneously into three 
different binary classes, which makes it a multioutput classifier.</p>

<figure class="smallersixty"><div id="perceptron_diagram" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1005.png" alt="mlst 1005" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1005.png" width="1378" height="791">
<h6><span class="label">Figure 10-5. </span>Perceptron diagram</h6>
</div></figure>

<p>So how is a <a data-type="indexterm" data-primary="Perceptrons" data-secondary="training" id="p10t"></a>Perceptron trained? The Perceptron training algorithm proposed by Frank Rosenblatt was largely inspired <a data-type="indexterm" data-primary="Hebb's rule" id="idm140583002193680"></a>by <em>Hebb’s rule</em>. In his book <em>The Organization of Behavior</em>,
 published in 1949, Donald Hebb suggested that when a biological neuron 
often triggers another neuron, the connection between these two neurons 
grows stronger. This idea was later summarized by Siegrid Löwel in this 
catchy phrase: “Cells that fire together, wire together.” This rule 
later became known as Hebb’s rule <a data-type="indexterm" data-primary="Hebbian learning" id="idm140583002191680"></a>(or <em>Hebbian learning</em>);
 that is, the connection weight between two neurons is increased 
whenever they have the same output. Perceptrons are trained using a 
variant of this rule that takes into account the error made by the 
network; it does not reinforce connections that lead to the wrong 
output. More specifically, the Perceptron is fed one training instance 
at a time, and for each instance it makes its predictions. For every 
output neuron that produced a wrong prediction, it reinforces the 
connection weights from the inputs that would have contributed to the 
correct prediction. The rule is shown in <a data-type="xref" href="#perceptron_update_rule">Equation 10-2</a>.</p>
<div id="perceptron_update_rule" data-type="equation">
<h5><span class="label">Equation 10-2. </span>Perceptron learning rule (weight update)</h5>
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/eq_99.png" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/eq_99.png" width="807" height="86">
</div>

<ul>
<li>
<p><em>w</em><sub><em>i</em>, <em>j</em></sub> is the connection weight between the i<sup>th</sup> input neuron and the j<sup>th</sup> output neuron.</p>
</li>
<li>
<p><em>x</em><sub><em>i</em></sub> is the i<sup>th</sup> input value of the current training instance.</p>
</li>
<li>
<p><img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/eq_100.png" alt="ModifyingAbove y With caret" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/eq_100.png" width="13" height="28"><sub><em>j</em></sub> is the output of the j<sup>th</sup> output neuron for the current training instance.</p>
</li>
<li>
<p><em>y</em><sub><em>j</em></sub> is the target output of the j<sup>th</sup> output neuron for the current training instance.</p>
</li>
<li>
<p><em>η</em> is the learning rate.</p>
</li>
</ul>

<p>The decision boundary of each output neuron is linear, so Perceptrons
 are incapable of learning complex patterns (just like Logistic 
Regression classifiers). However, if the training instances are linearly
 separable, Rosenblatt demonstrated that this algorithm would converge 
to a solution.<sup><a data-type="noteref" id="idm140583002121120-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002121120" class="totri-footnote">7</a></sup> This is <a data-type="indexterm" data-primary="Perceptrons" data-secondary="training" data-startref="p10t" id="idm140583002120256"></a>called <a data-type="indexterm" data-primary="Perceptron convergence theorem" id="idm140583002118880"></a>the <em>Perceptron convergence theorem</em>.</p>

<p>Scikit-Learn <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="Perceptron class" id="idm140583002117120"></a>provides a <code>Perceptron</code> class <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.datasets.load_iris()" id="idm140583002115568"></a><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.Perceptron" id="idm140583002114544"></a>that
 implements a single LTU network. It can be used pretty much as you 
would expect—for example, on the iris dataset (introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Perceptron</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:,</code> <code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">)]</code>  <code class="c1"># petal length, petal width</code>
<code class="n">y</code> <code class="o">=</code> <code class="p">(</code><code class="n">iris</code><code class="o">.</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">int</code><code class="p">)</code>  <code class="c1"># Iris Setosa?</code>

<code class="n">per_clf</code> <code class="o">=</code> <code class="n">Perceptron</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">per_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">y_pred</code> <code class="o">=</code> <code class="n">per_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mi">2</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">]])</code></pre>

<p>You may have recognized that the Perceptron learning algorithm strongly resembles <a data-type="indexterm" data-primary="Stochastic Gradient Descent (SGD)" id="idm140583002110064"></a>Stochastic Gradient Descent. In fact, Scikit-Learn’s <code>Perceptron</code> class is equivalent to using an <code>SGDClassifier</code> with the following hyperparameters: <code>loss="perceptron"</code>, <code>learning_rate="constant"</code>, <code>eta0=1</code> (the learning rate), and <code>penalty=None</code> (no regularization).</p>

<p>Note <a data-type="indexterm" data-primary="Perceptrons" data-secondary="versus Logistic Regression" data-secondary-sortas="Logistic" id="idm140583002034048"></a>that
 contrary to Logistic Regression classifiers, Perceptrons do not output a
 class probability; rather, they just make predictions based on a hard 
threshold. This is one of the good reasons to prefer Logistic Regression
 over Perceptrons.</p>

<p>In their 1969 monograph titled <em>Perceptrons</em>, Marvin Minsky 
and Seymour Papert highlighted a number of serious weaknesses of 
Perceptrons, in particular the fact that they are incapable of solving 
some trivial problems (e.g., the <em>Exclusive OR</em> (XOR) classification problem; see the left side of <a data-type="xref" href="#xor_diagram">Figure&nbsp;10-6</a>).
 Of course this is true of any other linear classification model as well
 (such as Logistic Regression classifiers), but researchers had expected
 much more from Perceptrons, and their disappointment was great: as a 
result, many researchers dropped <em>connectionism</em> <a data-type="indexterm" data-primary="connectionism" id="idm140583002029168"></a>altogether (i.e., the study of neural networks) in favor of higher-level problems such as logic, problem solving, and search.</p>

<p>However, it turns out that some of the limitations of Perceptrons can
 be eliminated by stacking multiple Perceptrons. The resulting ANN is 
called a <em>Multi-Layer Perceptron</em> (MLP). <a data-type="indexterm" data-primary="Multi-Layer Perceptrons (MLP)" id="mlp10"></a>In
 particular, an MLP can solve the XOR problem, as you can verify by 
computing the output of the MLP represented on the right of <a data-type="xref" href="#xor_diagram">Figure&nbsp;10-6</a>, for each combination of inputs: with inputs (0, 0) or (1, 1) the network outputs 0, and with <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="Perceptrons" data-secondary-sortas="Perceptron" data-startref="ann10tp" id="idm140583002025072"></a><a data-type="indexterm" data-primary="Perceptrons" data-startref="p10" id="idm140583002023568"></a>inputs (0, 1) or (1, 0) it outputs 1.</p>

<figure class="smallersixty"><div id="xor_diagram" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1006.png" alt="mlst 1006" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1006.png" width="1285" height="775">
<h6><span class="label">Figure 10-6. </span>XOR classification problem and an MLP that solves it</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Multi-Layer Perceptron and Backpropagation"><div class="sect2" id="idm140583002233744">
<h2>Multi-Layer Perceptron and Backpropagation</h2>

<p>An MLP is composed of one (passthrough) input layer, one or more layers of LTUs, called <em>hidden layers</em>, <a data-type="indexterm" data-primary="hidden layers" id="idm140583002018464"></a>and one final layer of LTUs called <a data-type="indexterm" data-primary="output layer" id="idm140583002017600"></a>the <em>output layer</em> (see <a data-type="xref" href="#mlp_diagram">Figure&nbsp;10-7</a>).
 Every layer except the output layer includes a bias neuron and is fully
 connected to the next layer. When an ANN has two or more hidden layers,
 it is called <a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-seealso="Multi-Layer Perceptrons (MLP)" id="idm140583002015344"></a>a <em>deep neural network</em> (DNN).</p>

<figure class="smallersixty"><div id="mlp_diagram" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1007.png" alt="mlst 1007" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1007.png" width="1300" height="926">
<h6><span class="label">Figure 10-7. </span>Multi-Layer Perceptron</h6>
</div></figure>

<p>For many years researchers struggled to find a way to train MLPs, 
without success. But in 1986, D. E. Rumelhart et al. published a <a href="https://goo.gl/Wl7Xyc">groundbreaking article</a><sup><a data-type="noteref" id="idm140583002010304-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002010304" class="totri-footnote">8</a></sup> introducing the <em>backpropagation</em> <a data-type="indexterm" data-primary="backpropagation" id="b10"></a>training algorithm.<sup><a data-type="noteref" id="idm140583002007936-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002007936" class="totri-footnote">9</a></sup> Today we would describe it as Gradient Descent using reverse-mode autodiff (Gradient Descent was introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>, and autodiff was discussed in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">Chapter&nbsp;9</a>).</p>

<p>For each training instance, the algorithm feeds it to the network and
 computes the output of every neuron in each consecutive layer (this is 
the forward pass, just like when making predictions). Then it measures 
the network’s output error (i.e., the difference between the desired 
output and the actual output of the network), and it computes how much 
each neuron in the last hidden layer contributed to each output neuron’s
 error. It then proceeds to measure how much of these error 
contributions came from each neuron in the previous hidden layer—and so 
on until the algorithm reaches the input layer. This reverse pass 
efficiently measures the error gradient across all the connection 
weights in the network by propagating the error gradient backward in the
 network (hence the name of the algorithm). If you check out the 
reverse-mode autodiff algorithm in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app04.html#autodiff_appendix">Appendix&nbsp;D</a>,
 you will find that the forward and reverse passes of backpropagation 
simply perform reverse-mode autodiff. The last step of the 
backpropagation algorithm is a Gradient Descent step on all the 
connection weights in the network, using the error gradients measured 
earlier.</p>

<p>Let’s make this even shorter: for each training instance the 
backpropagation algorithm first makes a prediction (forward pass), 
measures the error, then goes through each layer in reverse to measure 
the error contribution from each connection (reverse pass), and finally 
slightly tweaks the connection weights to reduce the <a data-type="indexterm" data-primary="backpropagation" data-startref="b10" id="idm140583002002368"></a>error (Gradient Descent step).</p>

<p>In order for this algorithm to work properly, the authors made a key 
change to the MLP’s architecture: they replaced the step function with 
the logistic function, <em>σ</em>(<em>z</em>) = 1 / (1 + exp(–<em>z</em>)).
 This was essential because the step function contains only flat 
segments, so there is no gradient to work with (Gradient Descent cannot 
move on a flat surface), while the logistic function has a well-defined 
nonzero derivative everywhere, allowing Gradient Descent to make some 
progress at every step. The backpropagation algorithm may be used with 
other <em>activation functions</em>, <a data-type="indexterm" data-primary="activation functions" id="af10"></a>instead of the logistic function. Two other popular activation functions are:</p>
<dl>
<dt>The <em>hyperbolic tangent</em> <a data-type="indexterm" data-primary="hyperbolic tangent (htan activation function)" id="idm140583001996000"></a>function tanh (<em>z</em>) = 2<em>σ</em>(2<em>z</em>) – 1</dt>
<dd>
<p>Just like the logistic function it is S-shaped, continuous, and 
differentiable, but its output value ranges from –1 to 1 (instead of 0 
to 1 in the case of the logistic function), which tends to make each 
layer’s output more or less normalized (i.e., centered around 0) at the 
beginning of training. This often helps speed up convergence.</p>
</dd>
<dt>The <a data-type="indexterm" data-primary="ReLU function" id="idm140583001992192"></a>ReLU function (introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">Chapter&nbsp;9</a>)</dt>
<dd>
<p>ReLU (<em>z</em>) = max (0, <em>z</em>). It is continuous but unfortunately not differentiable at <em>z</em>
 = 0 (the slope changes abruptly, which can make Gradient Descent bounce
 around). However, in practice it works very well and has the advantage 
of being fast to compute. Most importantly, the fact that it does not 
have a maximum output value also helps reduce some issues during 
Gradient Descent (we will come back to this in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">Chapter&nbsp;11</a>).</p>
</dd>
</dl>

<p>These popular activation functions and their derivatives are represented in <a data-type="xref" href="#activation_functions_plot">Figure&nbsp;10-8</a>.</p>

<figure><div id="activation_functions_plot" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1008.png" alt="mlst 1008" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1008.png" width="1440" height="494">
<h6><span class="label">Figure 10-8. </span>Activation functions and their derivatives</h6>
</div></figure>

<p>An MLP is often used for classification, with each output 
corresponding to a different binary class (e.g., spam/ham, 
urgent/not-urgent, and so on). When the classes are exclusive (e.g., 
classes 0 through 9 for digit image classification), the output layer is
 typically modified by replacing the individual activation functions by a
 shared <em>softmax</em> <a data-type="indexterm" data-primary="softmax function" id="idm140583001982304"></a>function (see <a data-type="xref" href="#fnn_for_classification_diagram">Figure&nbsp;10-9</a>). The softmax function was introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>.
 The output of each neuron corresponds to the estimated probability of 
the corresponding class. Note that the signal flows only in one 
direction (from the inputs to the outputs), so this architecture is an 
example <a data-type="indexterm" data-primary="feedforward neural network (FNN)" id="idm140583001979552"></a><a data-type="indexterm" data-primary="Multi-Layer Perceptrons (MLP)" data-startref="mlp10" id="idm140583001978864"></a>of a <em>feedforward neural network</em> (FNN).</p>

<figure class="smallerseventy"><div id="fnn_for_classification_diagram" class="figure">
<img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/mlst_1009.png" alt="mlst 1009" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1009.png" width="1449" height="1018">
<h6><span class="label">Figure 10-9. </span>A modern MLP (including ReLU and softmax) for classification</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Biological neurons seem to implement a roughly sigmoid (S-shaped) 
activation function, so researchers stuck to sigmoid functions for a 
very long time. But it turns out that the ReLU activation function 
generally works better in ANNs. This is one of the cases where the 
biological analogy was <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="Perceptrons" data-startref="ann10tp" id="idm140583001973920"></a><a data-type="indexterm" data-primary="Perceptrons" data-startref="p10" id="idm140583001972656"></a><a data-type="indexterm" data-primary="activation functions" data-startref="af10" id="idm140583001971712"></a>misleading.</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Training an MLP with TensorFlow’s High-Level API"><div class="sect1" id="idm140583002019968">
<h1>Training an MLP with TensorFlow’s High-Level API</h1>

<p>The <a data-type="indexterm" data-primary="Multi-Layer Perceptrons (MLP)" data-secondary="training with TF.Learn" id="idm140583001968784"></a><a data-type="indexterm" data-primary="TF.Learn" id="idm140583001967808"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="TF.Learn" id="idm140583001967136"></a><a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="training with TF.Learn" id="idm140583001966192"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.contrib.learn.DNNClassifier" id="idm140583001965280"></a>simplest
 way to train an MLP with TensorFlow is to use the high-level API 
TF.Learn, which offers a Scikit-Learn–compatible API. The <code>DNNClassifier</code> <a data-type="indexterm" data-primary="DNNClassifier" id="idm140583001963712"></a>class
 makes it fairly easy to train a deep neural network with any number of 
hidden layers, and a softmax output layer to output estimated class 
probabilities. For example, the following code trains a DNN for 
classification with two hidden layers (one with 300 neurons, and the 
other with 100 neurons) and a softmax output layer with 10 neurons:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>

<code class="n">feature_cols</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">contrib</code><code class="o">.</code><code class="n">learn</code><code class="o">.</code><code class="n">infer_real_valued_columns_from_input</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">dnn_clf</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">contrib</code><code class="o">.</code><code class="n">learn</code><code class="o">.</code><code class="n">DNNClassifier</code><code class="p">(</code><code class="n">hidden_units</code><code class="o">=</code><code class="p">[</code><code class="mi">300</code><code class="p">,</code><code class="mi">100</code><code class="p">],</code> <code class="n">n_classes</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                                         <code class="n">feature_columns</code><code class="o">=</code><code class="n">feature_cols</code><code class="p">)</code>
<code class="n">dnn_clf</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">contrib</code><code class="o">.</code><code class="n">learn</code><code class="o">.</code><code class="n">SKCompat</code><code class="p">(</code><code class="n">dnn_clf</code><code class="p">)</code>  <code class="c1"># if TensorFlow &gt;= 1.1</code>
<code class="n">dnn_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">steps</code><code class="o">=</code><code class="mi">40000</code><code class="p">)</code></pre>

<p>The code first creates a set of real valued columns from the training
 set (other types of columns, such as categorical columns, are 
available). Then we create the <code>DNNClassifier</code>, and we wrap 
it in a Scikit-Learn compatibility helper. Finally, we run 40,000 
training iterations using batches of 50 instances.</p>

<p>If <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.contrib.learn.infer_real_valued_columns_from_input()" id="idm140583001847904"></a>you run this code on the MNIST dataset (after scaling it, e.g., by using Scikit-Learn’s <code>StandardScaler</code>), <a data-type="indexterm" data-primary="StandardScaler" id="idm140583001846272"></a><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="StandardScaler" id="idm140583001845536"></a><a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.preprocessing.StandardScaler" id="idm140583001844592"></a>you will actually get a model that achieves around 98.2% accuracy on the test set! That’s better than the best model we <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.metrics.accuracy_score()" id="idm140583001843408"></a>trained in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch03.html#classification_chapter">Chapter&nbsp;3</a>:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">dnn_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">[</code><code class="s">'classes'</code><code class="p">])</code>
<code class="go">0.98250000000000004</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The <code>tensorflow.contrib</code> package <a data-type="indexterm" data-primary="tensorflow.contrib" id="idm140583001824192"></a>contains
 many useful functions, but it is a place for experimental code that has
 not yet graduated to be part of the core TensorFlow API. So the <code>DNNClassifier</code> class (and any other <code>contrib</code> code) may change without notice in the future.</p>
</div>

<p>Under the hood, the <code>DNNClassifier</code> class creates all the neuron layers, based on the ReLU activation function (we can change this by setting the <code>activation_fn</code> hyperparameter). The output layer relies on the softmax function, <a data-type="indexterm" data-primary="softmax function" id="idm140583001811696"></a><a data-type="indexterm" data-primary="cross entropy" id="idm140583001810992"></a>and the <a data-type="indexterm" data-primary="cost function" data-secondary="in artificial neural networks" data-secondary-sortas="artificial" id="idm140583001810192"></a>cost function is cross entropy (introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>).</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Training a DNN Using Plain TensorFlow"><div class="sect1" id="idm140583001807984">
<h1>Training a DNN Using Plain TensorFlow</h1>

<p>If <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="training a DNN with TensorFlow" id="ann10tadwt"></a><a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="training with TensorFlow" id="dnn10twtf"></a>you
 want more control over the architecture of the network, you may prefer 
to use TensorFlow’s lower-level Python API (introduced in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">Chapter&nbsp;9</a>).
 In this section we will build the same model as before using this API, 
and we will implement Mini-batch Gradient Descent to train it on the 
MNIST dataset. The first step is the construction phase, building the 
TensorFlow graph. The second step is the execution phase, where you 
actually run the graph to train the model.</p>








<section data-type="sect2" data-pdf-bookmark="Construction Phase"><div class="sect2" id="idm140583001802544">
<h2>Construction Phase</h2>

<p>Let’s start. <a data-type="indexterm" data-primary="TensorFlow" data-secondary="Python API" data-tertiary="construction" id="tf10papic"></a>First we need to import the <code>tensorflow</code> library. Then we must specify the number of inputs and outputs, and set the number of hidden neurons in each layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>

<code class="n">n_inputs</code> <code class="o">=</code> <code class="mi">28</code><code class="o">*</code><code class="mi">28</code>  <code class="c1"># MNIST</code>
<code class="n">n_hidden1</code> <code class="o">=</code> <code class="mi">300</code>
<code class="n">n_hidden2</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">n_outputs</code> <code class="o">=</code> <code class="mi">10</code></pre>

<p>Next, just like you did in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">Chapter&nbsp;9</a>, you can use placeholder nodes to represent the training data and targets. The shape of <code>X</code>
 is only partially defined. We know that it will be a 2D tensor (i.e., a
 matrix), with instances along the first dimension and features along 
the second dimension, and we know that the number of features is going 
to be 28 x 28 (one feature per pixel), but we don’t know yet how many 
instances each training batch will contain. So the shape of <code>X</code> is <code>(None, n_inputs)</code>. Similarly, we know that <code>y</code> will be a 1D tensor with one entry per instance, but again we don’t know the size of the training batch at this point, so the <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.int64" id="idm140583001660368"></a>shape is <code>(None)</code>.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="bp">None</code><code class="p">,</code> <code class="n">n_inputs</code><code class="p">),</code> <code class="n">name</code><code class="o">=</code><code class="s2">"X"</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">int64</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="bp">None</code><code class="p">),</code> <code class="n">name</code><code class="o">=</code><code class="s2">"y"</code><code class="p">)</code></pre>

<p>Now let’s create the actual neural network. The placeholder <code>X</code>
 will act as the input layer; during the execution phase, it will be 
replaced with one training batch at a time (note that all the instances 
in a training batch will be processed simultaneously by the neural 
network). Now you need to create the two hidden layers and the output 
layer. The two hidden layers are almost identical: they differ only by 
the inputs they are connected to and by the number of neurons they 
contain. The output layer is also very similar, but it uses a softmax 
activation function instead of a ReLU activation function. So let’s 
create a <code>neuron_layer()</code> function that we will use to create
 one layer at a time. It will need parameters to specify the inputs, the
 number of neurons, the activation function, <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.name_scope()" id="idm140583001761648"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.truncated_normal()" id="idm140583001760768"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.zeros()" id="idm140583001759824"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.matmul()" id="idm140583001758880"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.nn.relu()" id="idm140583001757936"></a>and the name of the layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">neuron_layer</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">n_neurons</code><code class="p">,</code> <code class="n">name</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="bp">None</code><code class="p">):</code>
    <code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">name_scope</code><code class="p">(</code><code class="n">name</code><code class="p">):</code>
        <code class="n">n_inputs</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">get_shape</code><code class="p">()[</code><code class="mi">1</code><code class="p">])</code>
        <code class="n">stddev</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">/</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">n_inputs</code> <code class="o">+</code> <code class="n">n_neurons</code><code class="p">)</code>
        <code class="n">init</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">truncated_normal</code><code class="p">((</code><code class="n">n_inputs</code><code class="p">,</code> <code class="n">n_neurons</code><code class="p">),</code> <code class="n">stddev</code><code class="o">=</code><code class="n">stddev</code><code class="p">)</code>
        <code class="n">W</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Variable</code><code class="p">(</code><code class="n">init</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"kernel"</code><code class="p">)</code>
        <code class="n">b</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Variable</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">n_neurons</code><code class="p">]),</code> <code class="n">name</code><code class="o">=</code><code class="s2">"bias"</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">W</code><code class="p">)</code> <code class="o">+</code> <code class="n">b</code>
        <code class="k">if</code> <code class="n">activation</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">activation</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">Z</code></pre>

<p>Let’s go through this code line by line:</p>
<ol>
<li>
<p>First we create a name scope using the name of the layer: it will 
contain all the computation nodes for this neuron layer. This is 
optional, but the graph will look much nicer in TensorBoard if its nodes
 are well organized.</p>
</li>
<li>
<p>Next, we get the number of inputs by looking up the input matrix’s 
shape and getting the size of the second dimension (the first dimension 
is for instances).</p>
</li>
<li>
<p>The next three lines create a <code>W</code> variable that will hold the weights matrix (often called the layer’s <em>kernel</em>). It will be a 2D tensor containing all the connection weights between each input and each neuron; hence, its shape will be <code>(n_inputs, n_neurons)</code>. It will be initialized randomly, using a truncated<sup><a data-type="noteref" id="idm140583001480640-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583001480640">10</a></sup> normal (Gaussian) distribution with a standard deviation of <img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/eq_101.png" data-mfp-src="/library/view/hands-on-machine-learning/9781491962282/assets/eq_101.png" width="181" height="30">. Using this specific standard deviation helps the algorithm converge much faster (we will discuss this further in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">Chapter&nbsp;11</a>;
 it is one of those small tweaks to neural networks that have had a 
tremendous impact on their efficiency). It is important to initialize 
connection weights randomly for all hidden layers to avoid any 
symmetries that the Gradient Descent algorithm would be unable to break.<sup><a data-type="noteref" id="idm140583001478144-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583001478144">11</a></sup></p>
</li>
<li>
<p>The next line creates a <code>b</code> variable for biases, initialized to 0 (no symmetry issue in this case), with one bias parameter per neuron.</p>
</li>
<li>
<p>Then we create a subgraph to compute <strong>Z</strong> = <strong>X</strong> · <strong>W</strong> + <strong>b</strong>.
 This vectorized implementation will efficiently compute the weighted 
sums of the inputs plus the bias term for each and every neuron in the 
layer, for all the instances in the batch in just one shot. Note that 
adding a 1D array (<strong>b</strong>) to a 2D matrix with the same number of columns (<strong>X</strong> . <strong>W</strong>) results in adding the 1D array to every row in the matrix: this is called <em>broadcasting</em>.</p>
</li>
<li>
<p>Finally, if an <code>activation</code> parameter is provided, such as <code>tf.nn.relu</code> <a data-type="indexterm" data-primary="relu(z)" id="idm140583001469216"></a>(i.e., max (0, <strong>Z</strong>)), then the code returns <code>activation(Z)</code>, or else it just returns <code>Z</code>.</p>
</li>

</ol>

<p>Okay, so now you have a nice function to create a neuron layer. Let’s
 use it to create the deep neural network! The first hidden layer takes <code>X</code>
 as its input. The second takes the output of the first hidden layer as 
its input. And finally, the output layer takes the output of the second 
hidden layer as its input.</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">name_scope</code><code class="p">(</code><code class="s2">"dnn"</code><code class="p">):</code>
    <code class="n">hidden1</code> <code class="o">=</code> <code class="n">neuron_layer</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">n_hidden1</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"hidden1"</code><code class="p">,</code>
                           <code class="n">activation</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">)</code>
    <code class="n">hidden2</code> <code class="o">=</code> <code class="n">neuron_layer</code><code class="p">(</code><code class="n">hidden1</code><code class="p">,</code> <code class="n">n_hidden2</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"hidden2"</code><code class="p">,</code>
                           <code class="n">activation</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">)</code>
    <code class="n">logits</code> <code class="o">=</code> <code class="n">neuron_layer</code><code class="p">(</code><code class="n">hidden2</code><code class="p">,</code> <code class="n">n_outputs</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"outputs"</code><code class="p">)</code></pre>

<p>Notice that once again we used a <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.name_scope()" id="tfnamescopech10"></a>name scope for clarity. Also note that <code>logits</code> is the output of the neural network <em>before</em> going through the softmax activation function: for optimization reasons, we will handle the softmax computation later.</p>

<p>As you might expect, TensorFlow comes with many handy functions to create <span class="keep-together">standard</span> neural network layers, so there’s often no need to define your own <span class="keep-together"><code>neuron_layer()</code></span> <a data-type="indexterm" data-primary="neuron_layer()" id="idm140583001287712"></a>function like we just did. For example, TensorFlow’s <code>tf.layers.dense()</code> <a data-type="indexterm" data-primary="dense()" id="idm140583001286432"></a>function (previously called <code>tf.contrib.layers.fully_connected()</code>) creates a fully connected layer, where all the inputs are connected to all the neurons in the <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.layers.dense()" id="idm140583001285072"></a>layer. It takes care of creating the <a data-type="indexterm" data-primary="weights" id="idm140583001283952"></a>weights and <a data-type="indexterm" data-primary="biases" id="idm140583001283152"></a>biases variables, named <code>kernel</code> and <code>bias</code> respectively, using the appropriate initialization strategy, and you can set the activation function using the <code>activation</code> argument. As we will see in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">Chapter&nbsp;11</a>, it also supports regularization parameters. Let’s tweak the preceding code to use the <code>dense()</code> function instead of our <code>neuron_layer()</code> function. Simply replace the <code>dnn</code> construction section with the following code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">name_scope</code><code class="p">(</code><code class="s2">"dnn"</code><code class="p">):</code>
    <code class="n">hidden1</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">dense</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">n_hidden1</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"hidden1"</code><code class="p">,</code>
                              <code class="n">activation</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">)</code>
    <code class="n">hidden2</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">dense</code><code class="p">(</code><code class="n">hidden1</code><code class="p">,</code> <code class="n">n_hidden2</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"hidden2"</code><code class="p">,</code>
                              <code class="n">activation</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">)</code>
    <code class="n">logits</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">dense</code><code class="p">(</code><code class="n">hidden2</code><code class="p">,</code> <code class="n">n_outputs</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"outputs"</code><code class="p">)</code></pre>

<p>Now that we have the neural network model ready to go, we need to define the <a data-type="indexterm" data-primary="cost function" data-secondary="in artificial neural networks" data-secondary-sortas="artificial" id="cfinannch10"></a>cost function that we will use to train it. Just as we did for Softmax Regression in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>,
 we will use cross entropy. As we discussed earlier, cross entropy will 
penalize models that estimate a low probability for the target class. 
TensorFlow provides <span class="keep-together">several</span> functions to compute cross entropy. <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.reduce_mean()" id="tfredmeanch10"></a> <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.nn.sparse_softmax_cross_entropy_with_logits()" id="tfnnsscewlch10"></a>We will use <code>sparse_softmax_cross_entropy_with_logits()</code>: it computes the cross entropy based on the “logits” (i.e., the output of the network <em>before</em>
 going through the softmax activation function), and it expects labels 
in the form of integers ranging from 0 to the number of classes minus 1 
(in our case, from 0 to 9). This will give us a 1D tensor containing the
 cross entropy for each instance. We can then use TensorFlow’s <code>reduce_mean()</code> function <a data-type="indexterm" data-primary="reduce_mean()" id="idm140583001381088"></a>to compute the mean cross entropy over all instances.</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">name_scope</code><code class="p">(</code><code class="s2">"loss"</code><code class="p">):</code>
    <code class="n">xentropy</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sparse_softmax_cross_entropy_with_logits</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">y</code><code class="p">,</code>
                                                              <code class="n">logits</code><code class="o">=</code><code class="n">logits</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">xentropy</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"loss"</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code>sparse_softmax_cross_entropy_with_logits()</code> function is equivalent <a data-type="indexterm" data-primary="sparse_softmax_cross_entropy_with_logits()" id="idm140583001071424"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.nn.sparse_softmax_cross_entropy_with_logits()" data-startref="tfnnsscewlch10" id="idm140583001070656"></a>to
 applying the softmax activation function and then computing the cross 
entropy, but it is more efficient, and it properly takes care of corner 
cases: when logits are large, floating-point rounding errors may cause 
the softmax output to be exactly equal to 0 or 1, and in this case the 
cross entropy equation would contain a log(0) term, equal to negative 
infinity. The <code>sparse_softmax_cross_​entropy_with_logits()</code> 
function solves this problem by computing log(ε) instead, where ε is a 
tiny positive number. This is why we did not apply the softmax 
activation function earlier. There is also another function called <code>softmax_cross_​entropy_with_logits()</code>, which takes labels in the form of one-hot vectors (instead of ints from 0 to the number of classes minus 1).</p>
</div>

<p>We have the neural network model, we have the <a data-type="indexterm" data-primary="cost function" data-secondary="in artificial neural networks" data-secondary-sortas="artificial" data-startref="cfinannch10" id="idm140583001066976"></a>cost function, and now we need to define a <code>GradientDescentOptimizer</code> that <a data-type="indexterm" data-primary="GradientDescentOptimizer" id="idm140583001225472"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.train.GradientDescentOptimizer" id="idm140583001224752"></a>will tweak the <a data-type="indexterm" data-primary="model parameters" id="idm140583001223664"></a>model parameters to minimize the cost function. <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.name_scope()" data-startref="tfnamescopech10" id="idm140583001222768"></a>Nothing new; it’s just like we did in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">Chapter&nbsp;9</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.01</code>

<code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">name_scope</code><code class="p">(</code><code class="s2">"train"</code><code class="p">):</code>
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">GradientDescentOptimizer</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">)</code>
    <code class="n">training_op</code> <code class="o">=</code> <code class="n">optimizer</code><code class="o">.</code><code class="n">minimize</code><code class="p">(</code><code class="n">loss</code><code class="p">)</code></pre>

<p>The last important step in the construction phase is to specify how to evaluate the model. <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.cast()" id="idm140583001029280"></a> <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.nn.in_top_k()" id="idm140583001028304"></a>We
 will simply use accuracy as our performance measure. First, for each 
instance, determine if the neural network’s prediction is correct by 
checking whether or not the highest logit corresponds to the target 
class. For this you can use the <code>in_top_k()</code> function. <a data-type="indexterm" data-primary="in_top_k()" id="idm140583001026592"></a>This
 returns a 1D tensor full of boolean values, so we need to cast these 
booleans to floats and then compute the average. This will give us the 
network’s overall accuracy.</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">name_scope</code><code class="p">(</code><code class="s2">"eval"</code><code class="p">):</code>
    <code class="n">correct</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">in_top_k</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">accuracy</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">correct</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">))</code></pre>

<p>And, as usual, <a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.reduce_mean()" data-startref="tfredmeanch10" id="idm140583001140688"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="tf.train.Saver" id="idm140583001182688"></a>we need to create a node to initialize all variables, and we will also create a <code>Saver</code> to save our trained model parameters to disk:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">init</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">()</code>
<code class="n">saver</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">Saver</code><code class="p">()</code></pre>

<p>Phew! This concludes the construction phase. This was fewer than 40 
lines of code, but it was pretty intense: we created placeholders for 
the inputs and the targets, we created a function to build a neuron 
layer, we used it to create the DNN, we defined the cost function, we 
created an optimizer, and finally we defined the performance measure. 
Now on to the <a data-type="indexterm" data-primary="TensorFlow" data-secondary="Python API" data-tertiary="construction" data-startref="tf10papic" id="idm140583000902992"></a>execution phase.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Execution Phase"><div class="sect2" id="idm140583001801632">
<h2>Execution Phase</h2>

<p>This <a data-type="indexterm" data-primary="TensorFlow" data-secondary="Python API" data-tertiary="execution" id="idm140583001174176"></a>part
 is much shorter and simpler. First, let’s load MNIST. We could use 
Scikit-Learn for that as we did in previous chapters, but TensorFlow 
offers its own helper that fetches the data, scales it (between 0 and 
1), shuffles it, and provides a simple function to load one mini-batch a
 time. Moreover, the data is already split into a training set (55,000 
instances), a validation set (5,000 instances), and a test set (10,000 
instances). So let’s use this helper:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tensorflow.examples.tutorials.mnist</code> <code class="kn">import</code> <code class="n">input_data</code>
<code class="n">mnist</code> <code class="o">=</code> <code class="n">input_data</code><code class="o">.</code><code class="n">read_data_sets</code><code class="p">(</code><code class="s2">"/tmp/data/"</code><code class="p">)</code></pre>

<p>Now we define the number of epochs that we want to run, as well as the size of the mini-batches:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_epochs</code> <code class="o">=</code> <code class="mi">40</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">50</code></pre>

<p>And now we can train the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>
    <code class="n">init</code><code class="o">.</code><code class="n">run</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">mnist</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">num_examples</code> <code class="o">//</code> <code class="n">batch_size</code><code class="p">):</code>
            <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">next_batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code>
            <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">training_op</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">X</code><code class="p">:</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="n">y_batch</code><code class="p">})</code>
        <code class="n">acc_train</code> <code class="o">=</code> <code class="n">accuracy</code><code class="o">.</code><code class="n">eval</code><code class="p">(</code><code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">X</code><code class="p">:</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="n">y_batch</code><code class="p">})</code>
        <code class="n">acc_val</code> <code class="o">=</code> <code class="n">accuracy</code><code class="o">.</code><code class="n">eval</code><code class="p">(</code><code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">X</code><code class="p">:</code> <code class="n">mnist</code><code class="o">.</code><code class="n">validation</code><code class="o">.</code><code class="n">images</code><code class="p">,</code>
                                           <code class="n">y</code><code class="p">:</code> <code class="n">mnist</code><code class="o">.</code><code class="n">validation</code><code class="o">.</code><code class="n">labels</code><code class="p">})</code>
        <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="s2">"Train accuracy:"</code><code class="p">,</code> <code class="n">acc_train</code><code class="p">,</code> <code class="s2">"Val accuracy:"</code><code class="p">,</code> <code class="n">acc_val</code><code class="p">)</code>

    <code class="n">save_path</code> <code class="o">=</code> <code class="n">saver</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">sess</code><code class="p">,</code> <code class="s2">"./my_model_final.ckpt"</code><code class="p">)</code></pre>

<p>This code opens a TensorFlow session, and it runs the <code>init</code>
 node that initializes all the variables. Then it runs the main training
 loop: at each epoch, the code iterates through a number of mini-batches
 that corresponds to the training set size. Each mini-batch is fetched 
via the <code>next_batch()</code> <a data-type="indexterm" data-primary="next_batch()" id="idm140583000886064"></a>method,
 and then the code simply runs the training operation, feeding it the 
current mini-batch input data and targets. Next, at the end of each 
epoch, the code evaluates the model on the last mini-batch and on the 
full validation set, and it prints out the result. Finally, the model 
parameters are saved to disk.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using the Neural Network"><div class="sect2" id="idm140583000805696">
<h2>Using the Neural Network</h2>

<p>Now <a data-type="indexterm" data-primary="TensorFlow" data-secondary="Python API" data-tertiary="using the neural network" id="idm140583000804432"></a>that the neural network is trained, you can use it to make predictions.
To do that, you can reuse the same construction phase, but change the execution phase like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>
    <code class="n">saver</code><code class="o">.</code><code class="n">restore</code><code class="p">(</code><code class="n">sess</code><code class="p">,</code> <code class="s2">"./my_model_final.ckpt"</code><code class="p">)</code>
    <code class="n">X_new_scaled</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># some new images (scaled from 0 to 1)</code>
    <code class="n">Z</code> <code class="o">=</code> <code class="n">logits</code><code class="o">.</code><code class="n">eval</code><code class="p">(</code><code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">X</code><code class="p">:</code> <code class="n">X_new_scaled</code><code class="p">})</code>
    <code class="n">y_pred</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>First the code loads the model parameters from disk. Then it loads 
some new images that you want to classify. Remember to apply the same 
feature scaling as for the training data (in this case, scale it from 0 
to 1). Then the code evaluates the <code>logits</code> node. If you wanted to know all the estimated class probabilities, you would need to apply the <code>softmax()</code>
 function to the logits, but if you just want to predict a class, you 
can simply pick the class that has the highest logit value <a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="training with TensorFlow" data-startref="dnn10twtf" id="idm140583000749520"></a><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="training a DNN with TensorFlow" data-startref="ann10tadwt" id="idm140583000748432"></a>(using the <code>argmax()</code> function does the trick).</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Fine-Tuning Neural Network Hyperparameters"><div class="sect1" id="idm140583000746384">
<h1>Fine-Tuning Neural Network Hyperparameters</h1>

<p>The <a data-type="indexterm" data-primary="neural network hyperparameters" id="nnh10"></a><a data-type="indexterm" data-primary="hyperparameters" data-seealso="neural network hyperparameters" id="idm140583000743824"></a><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="hyperparameter fine-tuning" id="ann10hft"></a>flexibility
 of neural networks is also one of their main drawbacks: there are many 
hyperparameters to tweak. Not only can you use any <a data-type="indexterm" data-primary="network topology" id="idm140583000741344"></a>imaginable <em>network topology</em>
 (how neurons are interconnected), but even in a simple MLP you can 
change the number of layers, the number of neurons per layer, the type 
of activation function to use in each layer, the weight initialization 
logic, and much more. How do you know what combination of 
hyperparameters is the best for your task?</p>

<p>Of course, you can use grid search with cross-validation to find the 
right hyperparameters, like you did in previous chapters, but since 
there are many hyperparameters to tune, and since training a neural 
network on a large dataset takes a lot of time, you will only be able to
 explore a tiny part of the hyperparameter space in a reasonable amount 
of time. It is much better to use <a href="https://goo.gl/QFjMKu">randomized search</a>, <a data-type="indexterm" data-primary="randomized search" id="idm140583000738304"></a>as we discussed in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch02.html#project_chapter">Chapter&nbsp;2</a>. Another option is to use a tool such as <a href="http://oscar.calldesk.ai/">Oscar</a>, which implements more complex algorithms to help you find a good set of hyperparameters quickly.</p>

<p>It helps to have an idea of what values are reasonable for each hyperparameter, so you can restrict the <a data-type="indexterm" data-primary="search space" id="idm140583000735216"></a>search space. Let’s start with the number of hidden layers.</p>








<section data-type="sect2" data-pdf-bookmark="Number of Hidden Layers"><div class="sect2" id="idm140583000734256">
<h2>Number of Hidden Layers</h2>

<p>For <a data-type="indexterm" data-primary="neural network hyperparameters" data-secondary="number of hidden layers" id="nnh10nohl"></a>many
 problems, you can just begin with a single hidden layer and you will 
get reasonable results. It has actually been shown that an MLP with just
 one hidden layer can model even the most complex functions provided it 
has enough neurons. For a long time, these facts convinced researchers 
that there was no need to investigate any deeper neural networks. But 
they overlooked the fact that deep networks have a much higher <em>parameter efficiency</em> <a data-type="indexterm" data-primary="parameter efficiency" id="idm140583000730480"></a>than
 shallow ones: they can model complex functions using exponentially 
fewer neurons than shallow nets, making them much faster to train.</p>

<p>To understand why, suppose you are asked to draw a forest using some 
drawing software, but you are forbidden to use copy/paste. You would 
have to draw each tree individually, branch per branch, leaf per leaf. 
If you could instead draw one leaf, copy/paste it to draw a branch, then
 copy/paste that branch to create a tree, and finally copy/paste this 
tree to make a forest, you would be finished in no time. Real-world data
 is often structured in such a hierarchical way and DNNs automatically 
take advantage of this fact: lower hidden layers model low-level 
structures (e.g., line segments of various shapes and orientations), 
intermediate hidden layers combine these low-level structures to model 
intermediate-level structures (e.g., squares, circles), and the highest 
hidden layers and the output layer combine these intermediate structures
 to model high-level structures (e.g., faces).</p>

<p>Not only does this hierarchical architecture help DNNs converge 
faster to a good solution, it also improves their ability to generalize 
to new datasets. For example, if you have already trained a model to 
recognize faces in pictures, and you now want to train a new neural 
network to recognize hairstyles, then you can kickstart training by 
reusing the lower layers of the first network. Instead of randomly 
initializing the weights and biases of the first few layers of the new 
neural network, you can initialize them to the value of the weights and 
biases of the lower layers of the first network. This way the network 
will not have to learn from scratch all the low-level structures that 
occur in most pictures; it will only have to learn the higher-level 
structures (e.g., hairstyles).</p>

<p>In summary, for many problems you can start with just one or two 
hidden layers and it will work just fine (e.g., you can easily reach 
above 97% accuracy on the MNIST dataset using just one hidden layer with
 a few hundred neurons, and above 98% accuracy using two hidden layers 
with the same total amount of neurons, in roughly the same amount of 
training time). For more complex problems, you can gradually ramp up the
 number of hidden layers, until you start overfitting the training set. 
Very complex tasks, such as large image classification or speech 
recognition, typically require networks with dozens of layers (or even 
hundreds, but not fully connected ones, as we will see in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch13.html#cnn_chapter">Chapter&nbsp;13</a>),
 and they need a huge amount of training data. However, you will rarely 
have to train such networks from scratch: it is much more common to 
reuse parts of a pretrained state-of-the-art network that performs a 
similar task. Training will be a lot faster and require much less <a data-type="indexterm" data-primary="neural network hyperparameters" data-secondary="number of hidden layers" data-startref="nnh10nohl" id="idm140583000724704"></a>data (we will discuss this in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">Chapter&nbsp;11</a>).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Number of Neurons per Hidden Layer"><div class="sect2" id="idm140583000722416">
<h2>Number of Neurons per Hidden Layer</h2>

<p>Obviously <a data-type="indexterm" data-primary="neural network hyperparameters" data-secondary="neurons per hidden layer" id="idm140583000721184"></a>the
 number of neurons in the input and output layers is determined by the 
type of input and output your task requires. For example, the MNIST task
 requires 28 x 28 = 784 input neurons and 10 output neurons. As for the 
hidden layers, a common practice is to size them to form a funnel, with 
fewer and fewer neurons at each layer—the rationale being that many 
low-level features can coalesce into far fewer high-level features. For 
example, a typical neural network for MNIST may have two hidden layers, 
the first with 300 neurons and the second with 100. However, this 
practice is not as common now, and you may simply use the same size for 
all hidden layers—for example, all hidden layers with 150 neurons: 
that’s just one hyperparameter to tune instead of one per layer. Just 
like for the number of layers, you can try increasing the number of 
neurons gradually until the network starts overfitting. In general you 
will get more bang for the buck by increasing the number of layers than 
the number of neurons per layer. Unfortunately, as you can see, finding 
the perfect amount of neurons is still somewhat of a black art.</p>

<p>A simpler approach is to pick a model with more layers and neurons than you actually need, then use <a data-type="indexterm" data-primary="early stopping" id="idm140583000718160"></a>early stopping to prevent it from <a data-type="indexterm" data-primary="overfitting" id="idm140583000717328"></a>overfitting (and other regularization techniques, especially <em>dropout</em>, as <a data-type="indexterm" data-primary="dropout" id="idm140583000716144"></a>we will see in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">Chapter&nbsp;11</a>). This has been dubbed the “stretch pants” approach:<sup><a data-type="noteref" id="idm140583000714448-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583000714448">12</a></sup>
 instead of wasting time looking for pants that perfectly match your 
size, just use large stretch pants that will shrink down to the right 
size.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Activation Functions"><div class="sect2" id="idm140583000712816">
<h2>Activation Functions</h2>

<p>In <a data-type="indexterm" data-primary="neural network hyperparameters" data-secondary="activation functions" id="idm140583000711488"></a>most cases you can use the <a data-type="indexterm" data-primary="ReLU function" id="idm140583000710336"></a>ReLU activation function in the hidden layers (or one of its variants, as we will see in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html#deep_chapter">Chapter&nbsp;11</a>).
 It is a bit faster to compute than other activation functions, and 
Gradient Descent does not get stuck as much on plateaus, thanks to the 
fact that it does not saturate for large input values (as opposed to the
 logistic function or the <a data-type="indexterm" data-primary="hyperbolic tangent (htan activation function)" id="idm140583000708368"></a>hyperbolic tangent function, which saturate at 1).</p>

<p>For the output layer, the softmax activation function is generally a 
good choice for classification tasks when the classes are mutually 
exclusive. When they are not mutually exclusive (or when there are just 
two classes), you generally want to use the logistic function. For 
regression tasks, you can simply use no activation function at all for 
the output layer.</p>

<p>This concludes this introduction to artificial neural networks. In 
the following chapters, we will discuss techniques to train very deep 
nets, and distribute training across multiple servers and GPUs. Then we 
will explore a few other popular neural network architectures: 
convolutional neural networks, recurrent neural networks, and <a data-type="indexterm" data-primary="neural network hyperparameters" data-startref="nnh10" id="idm140583000706064"></a><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="hyperparameter fine-tuning" data-startref="ann10hft" id="idm140583000705024"></a>autoencoders.<sup><a data-type="noteref" id="idm140583000703648-marker" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583000703648">13</a></sup></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm140583000701936">
<h1>Exercises</h1>
<ol>
<li>
<p>Draw an ANN using the original artificial neurons (like the ones in <a data-type="xref" href="#nn_propositional_logic_diagram">Figure&nbsp;10-3</a>) that computes <em>A</em> ⊕ <em>B</em> (where ⊕ represents the XOR operation). Hint: <em>A</em> ⊕ <em>B</em> = (<em>A</em> ∧ ¬ <em>B</em>) ∨ (¬ <em>A</em> ∧ <em>B</em>).</p>
</li>
<li>
<p>Why is it generally preferable to use a Logistic Regression 
classifier rather than a classical Perceptron (i.e., a single layer of 
linear threshold units trained using the Perceptron training algorithm)?
 How can you tweak a Perceptron to make it equivalent to a Logistic 
Regression classifier?</p>
</li>
<li>
<p>Why was the logistic activation function a key ingredient in training the first MLPs?</p>
</li>
<li>
<p>Name three popular activation functions. Can you draw them?</p>
</li>
<li>
<p>Suppose you have an MLP composed of one input layer with 10 
passthrough neurons, followed by one hidden layer with 50 artificial 
neurons, and finally one output layer with 3 artificial neurons. All 
artificial neurons use the ReLU activation function.</p>

<ul>
<li>
<p>What is the shape of the input matrix <strong>X</strong>?</p>
</li>
<li>
<p>What about the shape of the hidden layer’s weight vector <strong>W</strong><sub><em>h</em></sub>, and the shape of its bias vector <strong>b</strong><sub><em>h</em></sub>?</p>
</li>
<li>
<p>What is the shape of the output layer’s weight vector <strong>W</strong><sub><em>o</em></sub>, and its bias vector <strong>b</strong><sub><em>o</em></sub>?</p>
</li>
<li>
<p>What is the shape of the network’s output matrix <strong>Y</strong>?</p>
</li>
<li>
<p>Write the equation that computes the network’s output matrix <strong>Y</strong> as a function of <strong>X</strong>, <strong>W</strong><sub><em>h</em></sub>, <strong>b</strong><sub><em>h</em></sub>, <strong>W</strong><sub><em>o</em></sub> and <strong>b</strong><sub><em>o</em></sub>.</p>
</li>
</ul>
</li>
<li>
<p>How many neurons do you need in the output layer if you want to 
classify email into spam or ham? What activation function should you use
 in the output layer? If instead you want to tackle MNIST, how many 
neurons do you need in the output layer, using what activation function?
 Answer the same questions for getting your network to predict housing 
prices as in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch02.html#project_chapter">Chapter&nbsp;2</a>.</p>
</li>
<li>
<p>What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?</p>
</li>
<li>
<p>Can you list all the hyperparameters you can tweak in an MLP? If the 
MLP overfits the training data, how could you tweak these 
hyperparameters to try to solve the problem?</p>
</li>
<li>
<p>Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html#tensorflow_chapter">Chapter&nbsp;9</a>,
 try adding all the bells and whistles (i.e., save checkpoints, restore 
the last checkpoint in case of an interruption, add summaries, plot 
learning curves using TensorBoard, <a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-startref="ann10" id="idm140583000671136"></a>and so on).</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm140583002295248"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002295248-marker" class="totri-footnote">1</a></sup>
 You can get the best of both worlds by being open to biological 
inspirations without being afraid to create biologically unrealistic 
models, as long as they work well.</p><p data-type="footnote" id="idm140583002280320"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002280320-marker" class="totri-footnote">2</a></sup> “A Logical Calculus of Ideas Immanent in Nervous Activity,” W. McCulloch and W. Pitts (1943).</p><p data-type="footnote" id="idm140583002259840"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002259840-marker" class="totri-footnote">3</a></sup> Image by Bruce Blaus (<a href="https://creativecommons.org/licenses/by/3.0/">Creative Commons 3.0</a>). Reproduced from <a href="https://en.wikipedia.org/wiki/Neuron"><em class="hyperlink">https://en.wikipedia.org/wiki/Neuron</em></a>.</p><p data-type="footnote" id="idm140583002256368"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002256368-marker" class="totri-footnote">4</a></sup> In the context of Machine Learning, the phrase “neural networks” generally refers to ANNs, not BNNs.</p><p data-type="footnote" id="idm140583002250368"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002250368-marker" class="totri-footnote">5</a></sup> Drawing of a cortical lamination by S.&nbsp;Ramon y Cajal (public domain). Reproduced from <a href="https://en.wikipedia.org/wiki/Cerebral_cortex"><em class="hyperlink">https://en.wikipedia.org/wiki/Cerebral_cortex</em></a>.</p><p data-type="footnote" id="idm140583002204192"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002204192-marker" class="totri-footnote">6</a></sup> The name <em>Perceptron</em> is sometimes used to mean a tiny network with a single LTU.</p><p data-type="footnote" id="idm140583002121120"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002121120-marker" class="totri-footnote">7</a></sup>
 Note that this solution is generally not unique: in general when the 
data are linearly separable, there is an infinity of hyperplanes that 
can separate them.</p><p data-type="footnote" id="idm140583002010304"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002010304-marker" class="totri-footnote">8</a></sup> “Learning Internal Representations by Error Propagation,” D. Rumelhart, G. Hinton, R. Williams (1986).</p><p data-type="footnote" id="idm140583002007936"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583002007936-marker" class="totri-footnote">9</a></sup>
 This algorithm was actually invented several times by various 
researchers in different fields, starting with P.&nbsp;Werbos in 1974.</p><p data-type="footnote" id="idm140583001480640"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583001480640-marker">10</a></sup>
 Using a truncated normal distribution rather than a regular normal 
distribution ensures that there won’t be any large weights, which could 
slow down training.</p><p data-type="footnote" id="idm140583001478144"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583001478144-marker">11</a></sup>
 For example, if you set all the weights to 0, then all neurons will 
output 0, and the error gradient will be the same for all neurons in a 
given hidden layer. The Gradient Descent step will then update all the 
weights in exactly the same way in each layer, so they will all remain 
equal. In other words, despite having hundreds of neurons per layer, 
your model will act as if there were only one neuron per layer. It is 
not going to fly.</p><p data-type="footnote" id="idm140583000714448"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583000714448-marker">12</a></sup> By Vincent Vanhoucke in his <a href="https://goo.gl/Y5TFqz">Deep Learning class</a> on Udacity.com.</p><p data-type="footnote" id="idm140583000703648"><sup><a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch10.html#idm140583000703648-marker">13</a></sup> A few extra ANN architectures are presented in <a data-type="xref" href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/app05.html#other_ann_appendix">Appendix&nbsp;E</a>.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-14" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="#">Copy</a></li>
		
		<li class="add-highlight"><a href="#">Add Highlight</a></li>
		<li class="add-note"><a href="#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch09.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">9. Up and Running with TensorFlow</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch11.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">11. Training Deep Neural Nets</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 69px;">
    <a href="#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/s/">Queue</a>
      
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2017 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"agent":"","errorBeacon":"bam.nr-data.net","licenseKey":"510f1a6865","queueTime":0,"beacon":"bam.nr-data.net","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","applicationID":"3275661","applicationTime":811}</script>


    

    <script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a_005.js" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.31551976328735265"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.7746668699298912" alt="" src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/0.txt" width="0" height="0"></div>
    <script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a_004.js" charset="utf-8"></script>
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 200px; left: 1257px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="#">Reset</a>
</div>
</div><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a.js" type="text/javascript"></script><script src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/a" type="text/javascript"></script><img src="10.%20Introduction%20to%20Artificial%20Neural%20Networks%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow_files/seg.gif" alt="" style="display: none;" width="1" height="1" border="0"></body></html>